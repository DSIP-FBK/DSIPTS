dataset:
  dataset: 'temps'  
  path: '/storage3/DSIP/agobbi/Projects/ExpTS/data'

scheduler_config:
  gamma: 0.75
  step_size: 2500

optim_config:
  lr: 0.00005
  weight_decay: 0.0001 #001

model_configs:
  past_steps: 1095
  future_steps: 120
  quantiles: [] #[0.1,0.5,0.9]
  past_channels : null #dataset dependent
  future_channels : null #dataset dependent
  embs: null #dataset dependent
  out_channels: null #dataset dependent
  loss_type: null
  persistence_weight: 1.0

split_params:
  perc_train: 0.6
  perc_valid: 0.2
  range_train: null
  range_validation: null
  range_test: null
  shift: 0
  starting_point: null
  skip_step: 1
  past_steps: model_configs@past_steps 
  future_steps: model_configs@future_steps


train_config:
  dirpath: "/storage3/DSIP/agobbi/Projects/ExpTS/temps" 
  num_workers: 0
  auto_lr_find: true
  devices: [0]                   
  seed: 42    

inference:
  output_path: "/storage3/DSIP/agobbi/Projects/ExpTS/temps" 
  load_last: true
  batch_size: 200 
  num_workers: 4
  set: "test"
  rescaling: true

defaults:
  - _self_
  - architecture: null 
  - override hydra/launcher: submitit_slurm #joblib 

hydra:
  launcher:
    submitit_folder: ${hydra.sweep.dir}/.submitit/%j
    timeout_min: 8000
    partition: gpu-K80 #A40 #gpu-V100
    mem_gb: 6
    nodes: 1
    gres: gpu:1
    array_parallelism: 10 ##altrimenti roldano si arrabbia
    name: ${hydra.job.name}
    _target_: hydra_plugins.hydra_submitit_launcher.submitit_launcher.SlurmLauncher
    setup:
      - conda activate tt  
 

  output_subdir: null 
  sweeper:
    params:
      architecture: glob(*) 



#python train.py  --config-dir=config_temps --config-name=config_slurm -m architecture=autoformer model_configs.loss_type=mse,exponential_penalization,linear_penalization model_configs.d_model=64 model_configs.n_layer_encoder=4 model_configs.n_layer_decoder=4 model_configs.hidden_size=64