{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8f4456-39e8-4bac-9514-dac172858380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dsipts import TimeSeries, RNN,read_public_dataset, LinearTS, Persistent, TFT\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "import sys\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98dc5459-3670-4357-8059-f3cccade6431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_walk(n,level=20):\n",
    "    tot = np.zeros(n)\n",
    "    probs = np.zeros(n)\n",
    "    for i in range(n-1):\n",
    "        prob = random.random()\n",
    "        if prob<0.5:\n",
    "            delta = 1\n",
    "        else:\n",
    "            delta = -1\n",
    "        tot[i+1] = tot[i] + delta\n",
    "        probs[i+1]= prob +(random.random()/level-0.5/level)\n",
    "    return tot, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843f0c90-c4aa-4118-a167-e8e409c53092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 20000\n",
    "random.seed(6)\n",
    "x, p = random_walk(N)\n",
    "data = pd.DataFrame({'y':x/x.max(),'p':p,'time':range(N)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe218d82-40f9-43c9-8118-818249679ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the timeseries to the datastructure, adding the hour column and use all the covariates\n",
    "ts = TimeSeries('weather')\n",
    "ts.load_signal(data,enrich_cat=[],target_variables=['y'],past_variables= [],future_variables=['p'])\n",
    "ts.set_verbose(False)\n",
    "#Let now prepare a model predictin the next 16 step using the past 16 steps \n",
    "past_steps = 64\n",
    "future_steps = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e51045-5762-455c-9998-6129b3c7de46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_params = {'perc_train':0.6,'perc_valid':0.2,                             ##if not None it will split 70% 10% 20%\n",
    "               'range_train':None, 'range_validation':None, 'range_test':None, ## or we can split using ranges for example range_train=['2021-02-03','2022-04-08']\n",
    "               'past_steps':past_steps,\n",
    "               'future_steps':future_steps,\n",
    "               'shift':0,\n",
    "               'starting_point':None,                                          ## do not skip samples\n",
    "               'skip_step' : 1                                                 ## distance between two consecutive samples\n",
    "                             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5ed48-70d2-45d4-9c21-4a5eb8122290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/agobbi/Projects/ExpTS/rf/linear exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3677799258e24863b7b0b7b8d174aa60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 93 steps due to diverging loss.\n",
      "Learning rate set to 0.04365158322401657\n",
      "Restoring states from the checkpoint path at /home/agobbi/Projects/ExpTS/rf/linear/.lr_find_d7737c14-ddb0-4aae-b553-392206e085c5.ckpt\n",
      "Restored all states from the checkpoint file at /home/agobbi/Projects/ExpTS/rf/linear/.lr_find_d7737c14-ddb0-4aae-b553-392206e085c5.ckpt\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | embs   | ModuleList | 0     \n",
      "1 | loss   | L1Loss     | 0     \n",
      "2 | linear | ModuleList | 28.5 K\n",
      "--------------------------------------\n",
      "28.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "28.5 K    Total params\n",
      "0.114     Total estimated model params size (MB)\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving losses on file because multigpu not working\n",
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/agobbi/Projects/ExpTS/rf/linear exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c1d849c5964d79a9752aae35e4a600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 91 steps due to diverging loss.\n",
      "Learning rate set to 0.04365158322401657\n",
      "Restoring states from the checkpoint path at /home/agobbi/Projects/ExpTS/rf/linear/.lr_find_62691845-edd0-45d8-b29b-81babeb0ccb8.ckpt\n",
      "Restored all states from the checkpoint file at /home/agobbi/Projects/ExpTS/rf/linear/.lr_find_62691845-edd0-45d8-b29b-81babeb0ccb8.ckpt\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | embs   | ModuleList | 0     \n",
      "1 | loss   | L1Loss     | 0     \n",
      "2 | linear | ModuleList | 28.5 K\n",
      "--------------------------------------\n",
      "28.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "28.5 K    Total params\n",
      "0.114     Total estimated model params size (MB)\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving losses on file because multigpu not working\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/agobbi/Projects/ExpTS/rf/linear exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf1b45b34cc4769aed2b3aeeb0dc842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 92 steps due to diverging loss.\n",
      "Learning rate set to 0.036307805477010104\n",
      "Restoring states from the checkpoint path at /home/agobbi/Projects/ExpTS/rf/linear/.lr_find_4f5542e9-8183-4579-bdfe-c0d93f73d3ab.ckpt\n",
      "Restored all states from the checkpoint file at /home/agobbi/Projects/ExpTS/rf/linear/.lr_find_4f5542e9-8183-4579-bdfe-c0d93f73d3ab.ckpt\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | embs   | ModuleList | 0     \n",
      "1 | loss   | L1Loss     | 0     \n",
      "2 | linear | ModuleList | 28.5 K\n",
      "--------------------------------------\n",
      "28.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "28.5 K    Total params\n",
      "0.114     Total estimated model params size (MB)\n",
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "res_tot = {}\n",
    "for w in np.arange(0,1,0.25):\n",
    "    print(w)\n",
    "    config = dict(model_configs =dict(\n",
    "                                        past_steps = past_steps,\n",
    "                                        future_steps = future_steps,\n",
    "                                        past_channels = len(ts.past_variables),\n",
    "                                        future_channels = len(ts.future_variables),\n",
    "                                        embs = [ts.dataset[c].nunique() for c in ts.cat_var],\n",
    "                                        cat_emb_dim = 8,\n",
    "                                        kernel_size = 3,\n",
    "                                         use_bn = False,\n",
    "                                        dropout_rate=0.25,\n",
    "                                          optim='torch.optim.Adam',\n",
    "                                          activation= 'torch.nn.PReLU',\n",
    "                                         sum_emb = True,\n",
    "                                         out_channels = len(ts.target_variables),\n",
    "                                        hidden_size = 128,\n",
    "                                        kind='linear',\n",
    "                                        quantiles= [],\n",
    "                                        persistence_weight =w,\n",
    "                                        simple=False,loss_type='exponential_penalization'\n",
    "                                        ),\n",
    "                    scheduler_config = dict(gamma=0.1,step_size=24000000000000000),\n",
    "                    optim_config = dict(lr = 0.0005,weight_decay=0.00))\n",
    "    model_linear = LinearTS(**config['model_configs'],optim_config = config['optim_config'],scheduler_config =config['scheduler_config'],verbose=False )\n",
    "    ts.set_model(model_linear,config=config )\n",
    "    #train the model for 50 epochs with auto_lr_find \n",
    "    ts.train_model(dirpath=f\"/home/agobbi/Projects/ExpTS/rf/linear\",\n",
    "                   split_params=split_params,\n",
    "                   batch_size=32,\n",
    "                   num_workers=2,\n",
    "                   max_epochs=200,\n",
    "                   auto_lr_find=True)\n",
    "    res = ts.inference_on_set(200,4,set='test',rescaling=True)\n",
    "    res_tot[w] = res.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bc407-7cf1-4f41-9d89-4ed5e4ce478f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts.losses.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26975a6a-593b-4404-ac09-09dce13b3e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "error = {}\n",
    "for k in res_tot.keys():\n",
    "    error[k] = res_tot[k].groupby('lag').apply(lambda x: np.sqrt(np.mean((x.y_pred-x.y)**2))).reset_index()\n",
    "    plt.plot(error[k].lag, error[k][0],label=k)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef4256-7518-4418-878f-a7f3681be2ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e439300-d754-4ee7-a7c3-4dc4891e8bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f5fef-92e5-4e6a-81c7-21cc0f39d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.5 --  0.026778758\n",
    "#0   -- 0.027390912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d311e-c340-4e9e-8340-3f5e7d129201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##0 -- 0.026814144\n",
    "##2 -- 0.026652765\n",
    "##5 -- 0.026685152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0e384-596d-4f78-b867-36add5ce63a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i,k in enumerate(res_tot.keys()):\n",
    "    res_tot[k]['prediction_time'] = res_tot[k].apply(lambda x: int(x.time-x.lag), axis=1)\n",
    "    if i ==0:\n",
    "        plt.plot( res_tot[k][ res_tot[k].prediction_time==date].time,  res_tot[k][ res_tot[k].prediction_time==date].y,label='real',alpha=0.5)\n",
    "    plt.plot( res_tot[k][ res_tot[k].prediction_time==date].time,  res_tot[k][ res_tot[k].prediction_time==date].y_pred,label=k,alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e04216-3178-4b2f-86ec-3abaad372424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#plt.ylim(res.y.min(),res.y.max())\n",
    "plt.title('Prediction on test for lag=7')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41c3a1-4108-4d5e-a81f-baaaa00bcf26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get the median MSE for each lag\n",
    "import numpy as np\n",
    "res.groupby('lag').apply(lambda x: np.nanmean((x.y-x.y_median)**2)).reset_index().rename(columns={0:'error'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abe2ae-d010-4e95-b712-8e4084639d17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save model \n",
    "ts.save(f\"{model_to_use}_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f7417-48ea-4498-8724-89fb1eedccf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## load the model and check if we obtain the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1712bce-071f-4ea9-92bd-e4f4637ebb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.load(LinearTS,f\"{model_to_use}_test\",load_last=False)\n",
    "res = ts.inference_on_set(200,4,set='test',rescaling=True)\n",
    "error = res.groupby('lag').apply(lambda x: np.nanmean((x.y-x.y_median)**2)).reset_index().rename(columns={0:'error'}) \n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bcd30c-d798-4a15-8f8c-7587c807d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "##print the mean MSE along the lag steps\n",
    "plt.plot(error.lag,error.error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0f069-dc3c-4b16-9ede-747df97721f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "lag = 7\n",
    "try:\n",
    "    %matplotlib qast\n",
    "    to_plot = res\n",
    "except:\n",
    "    print('better to have qt, i will reduce the dataset')\n",
    "    plt.figure(figsize=(15,7))\n",
    "    to_plot = res[res.time>pd.to_datetime('2020-12-28')]\n",
    "plt.plot(to_plot[to_plot.lag==lag].time, to_plot[to_plot.lag==lag].y,label='real',alpha=0.5)\n",
    "plt.plot(to_plot[to_plot.lag==lag].time, to_plot[to_plot.lag==lag].y_median,label='median',alpha=0.5)\n",
    "plt.fill_between(to_plot[to_plot.lag==lag].time, to_plot[to_plot.lag==lag].y_low , to_plot[to_plot.lag==lag].y_high, alpha=0.2,label='error band')\n",
    "\n",
    "plt.title('Prediction on test for lag=7')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6fd90-b6ae-4763-8d51-cb05de421797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot = pd.read_csv('/home/agobbi/Projects/ExpTS/csv/prova_test_tot_predictions.csv')\n",
    "tot.time = pd.to_datetime(tot.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5246d-6540-4817-a342-bc164514ff29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pers = tot[(tot.model=='persistent_weather_1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f74643-7f33-411f-ba84-b046616ccb84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "lag = 7\n",
    "try:\n",
    "    %matplotlib qast\n",
    "    to_plot = pers\n",
    "except:\n",
    "    print('better to have qt, i will reduce the dataset')\n",
    "    plt.figure(figsize=(15,7))\n",
    "    to_plot = pers[pers.time>pd.to_datetime('2020-12-28')]\n",
    "plt.plot(to_plot[to_plot.lag==lag].time, to_plot[to_plot.lag==lag].y,label='real',alpha=0.5)\n",
    "plt.plot(to_plot[to_plot.lag==lag].time, to_plot[to_plot.lag==lag].y_pred,label='median',alpha=0.5)\n",
    "plt.fill_between(to_plot[to_plot.lag==lag].time, to_plot[to_plot.lag==lag].y_low , to_plot[to_plot.lag==lag].y_high, alpha=0.2,label='error band')\n",
    "\n",
    "plt.title('Prediction on test for lag=7')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d8600-b39d-42b1-8ed4-b1cc198c2da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
