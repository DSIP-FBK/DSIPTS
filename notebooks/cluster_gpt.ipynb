{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6b7188-2dd1-4f95-89e8-e08e1cca8fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "    from sklearn.cluster import BisectingKMeans\n",
    "import pickle\n",
    "from dsipts import read_public_dataset, TimeSeries\n",
    "data, columns = read_public_dataset(  dataset= 'weather', path= '/home/agobbi/Projects/ExpTS/data')\n",
    "ts = TimeSeries('prova')\n",
    "use_covariates = False\n",
    "ts.load_signal(data, enrich_cat=  ['hour'],target_variables=['y'], past_variables=columns if use_covariates else [])\n",
    "train,validation,test= ts.split_for_train(  perc_train= 0.8,  perc_valid= 0.1,  shift= 0,  skip_step=1,past_steps=16,future_steps=16)\n",
    "token_split = 4\n",
    "max_voc_size = 256\n",
    "samples,length,_ = train.data['y'].shape\n",
    "tmp = train.data['x_num_past'].squeeze().reshape(samples,-1,token_split)\n",
    "_,sentence_length, _ = tmp.shape\n",
    "tmp = tmp.reshape(-1,token_split)\n",
    "with open('cluster_model.pkl','rb') as f:\n",
    "    cl = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03cbcb6e-9bc4-45d8-9b39-73e7640174ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusters = cl.predict(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cf252fb-e6f4-40fb-b3a5-795517a19fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "samples = len(train.data['x_num_past'])\n",
    "tmp=train.data['x_num_past'].squeeze().reshape(-1,token_split)\n",
    "samples = len(test.data['y'])\n",
    "y_test= cl.predict(test.data['y'].reshape(-1,token_split)).reshape(samples,token_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cc05c521-86be-46eb-9068-bfcc010dbb7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('res.pkl','rb') as f:\n",
    "    real,predicted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "23a135c5-9958-47ea-8ecf-36b0d0ef824c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot = []\n",
    "for i in [1,2]:\n",
    "    res = []\n",
    "    data = tmp[np.where(clusters==i)[0]]\n",
    "    for j in range(data.shape[1]):\n",
    "\n",
    "        bootstrap_ci = bootstrap((data[:,j],), np.median,n_resamples=50, confidence_level=0.9,random_state=1, method='percentile')\n",
    "        res.append([bootstrap_ci.confidence_interval.low,np.median(data[:,j]),bootstrap_ci.confidence_interval.high])\n",
    "    tot.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "59ab90da-fcae-4f03-8c24-fb553504f5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import bootstrap\n",
    "tot_res = []\n",
    "for j in range(len(y_test)):\n",
    "    if j%100==0:\n",
    "        print(j)\n",
    "    res = []\n",
    "\n",
    "    sample = predicted[j]\n",
    "    for index in sample:\n",
    "        data=tmp[np.where(clusters==index)[0]]\n",
    "\n",
    "        for i in range(data.shape[1]):\n",
    "\n",
    "            bootstrap_ci = bootstrap((data[:,i],), np.median,n_resamples=50, confidence_level=0.9,\n",
    "                                     random_state=1, method='percentile')\n",
    "\n",
    "            res.append([bootstrap_ci.confidence_interval.low,np.median(data[:,i]),bootstrap_ci.confidence_interval.high])\n",
    "    tot_res.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "87465559-f7a3-4d5c-9412-00d43fd08cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot_res = np.stack(tot_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3f115a96-dbc7-4c96-97d8-7b763e87fc26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1ce3384b20>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "lag = 15\n",
    "plt.plot(tot_res[:,lag,1])\n",
    "plt.plot(test.data['y'][:,lag,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7c974400-2797-4464-bdde-b7eee04d84dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agobbi/.conda/envs/tt/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from dsipts import TimeSeries,read_public_dataset\n",
    "from sklearn.cluster import BisectingKMeans\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from minigpt import GPT\n",
    "from trainer import Trainer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import argparse\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "import hydra\n",
    "import pickle\n",
    "import numpy as np\n",
    "class SortDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset for the Sort problem. E.g. for problem length 6:\n",
    "    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 0 2 1 0 1 0 0 0 1 1\n",
    "    output: I I I I I 0 0 0 1 1 2\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,x,y,length, num_digits):\n",
    "        self.length = length\n",
    "        self.num_digits = num_digits\n",
    "        self.x = torch.tensor(x).long()\n",
    "        self.y = torch.tensor(y).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x) # ...\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.num_digits\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        return self.length * 2 - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        inp = self.x[idx]\n",
    "        sol = self.y[idx]\n",
    "        cat = torch.cat((inp, sol), dim=0)\n",
    "\n",
    "        # the inputs to the transformer will be the offset sequence\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:self.length-1] = -1\n",
    "        return x, y\n",
    "    \n",
    "data, columns = read_public_dataset(  dataset= 'weather', path= '/home/agobbi/Projects/ExpTS/data')\n",
    "ts = TimeSeries('prova')\n",
    "use_covariates = False\n",
    "ts.load_signal(data, enrich_cat=  ['hour'],target_variables=['y'], past_variables=columns if use_covariates else [])\n",
    "train,validation,test= ts.split_for_train(  perc_train= 0.8,  perc_valid= 0.1,  shift= 0,  skip_step=1,past_steps=16,future_steps=16)\n",
    "token_split = 4\n",
    "max_voc_size = 256\n",
    "samples,length,_ = train.data['y'].shape\n",
    "tmp = train.data['x_num_past'].squeeze().reshape(samples,-1,token_split)\n",
    "_,sentence_length, _ = tmp.shape\n",
    "tmp = tmp.reshape(-1,token_split)\n",
    "cl = BisectingKMeans(n_clusters=max_voc_size)\n",
    "clusters = cl.fit_predict(tmp)\n",
    "\n",
    "x_train = clusters.reshape(-1,sentence_length)\n",
    "samples = train.data['y'].shape[0]\n",
    "y_train = cl.predict(train.data['y'].squeeze().reshape(samples,-1,token_split).reshape(-1,token_split)).reshape(-1,sentence_length)\n",
    "samples = validation.data['y'].shape[0]\n",
    "y_validation = cl.predict(validation.data['y'].squeeze().reshape(samples,-1,token_split).reshape(-1,token_split)).reshape(-1,sentence_length)\n",
    "x_validation = cl.predict(validation.data['x_num_past'].squeeze().reshape(samples,-1,token_split).reshape(-1,token_split)).reshape(-1,sentence_length)\n",
    "\n",
    "samples = test.data['y'].shape[0]\n",
    "y_test = cl.predict(test.data['y'].squeeze().reshape(samples,-1,token_split).reshape(-1,token_split)).reshape(-1,sentence_length)\n",
    "x_test = cl.predict(test.data['x_num_past'].squeeze().reshape(samples,-1,token_split).reshape(-1,token_split)).reshape(-1,sentence_length)\n",
    "\n",
    "\n",
    "train_dataset = SortDataset(x_train,y_train,sentence_length,max_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "735ba47e-8a9e-4ce0-a01e-2bd104478b24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41979"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "374ab004-8b44-48c5-a6ff-591fb0573387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([194, 163, 174, 176, 160, 168, 175]),\n",
       " tensor([ -1,  -1,  -1, 160, 168, 175, 160]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "97c0d5f7-7ff6-4c95-9048-75297a68f391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=torch.utils.data.RandomSampler(train_dataset, replacement=True, num_samples=int(1e10)),\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "292bda2d-864e-4494-b4c6-fd2397995cba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 7])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d7cc3f18-e552-4895-8a2a-61c4b023b541",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encodings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mencodings\u001b[49m\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),encodings\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encodings' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cat([encodings.argmax(dim=2),encodings.argmax(dim=2)[:,0:-1]]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b344f0-7365-4ca2-8fbc-1d72c19e916d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
