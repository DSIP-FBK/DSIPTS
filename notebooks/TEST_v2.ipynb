{
 "cells": [
  {
   "cell_type": "raw",
   "id": "55c75749-ec70-4365-ac8a-f96552da4528",
   "metadata": {},
   "source": [
    "from dsipts import Categorical,TimeSeries, RNN\n",
    "settimana = Categorical('settimanale',1,[1,1,1,1,1,1,1],7,'multiplicative',[0.9,0.8,0.7,0.6,0.5,0.99,0.99])\n",
    "\n",
    "##montly, additive (here there are only 5 month)\n",
    "mese = Categorical('mensile',1,[31,28,20,10,33],5,'additive',[10,20,-10,20,0])\n",
    "\n",
    "##spot categorical variables: in this case it occurs every 100 days and it lasts 7 days adding 10 to the original timeseries\n",
    "spot = Categorical('spot',100,[7],1,'additive',[10])\n",
    "\n",
    "##initizate a timeseries object\n",
    "ts = TimeSeries('prova')\n",
    "ts.generate_signal(noise_mean=1,categorical_variables=[settimana,mese,spot],length=5000,type=0)\n",
    "dataset = ts.dataset\n",
    "dataset['x_num'] = 0.0\n",
    "dataset.loc[:dataset.shape[0]-10,'x_num']= dataset['x_num'].values[9:]\n",
    "cat_var_past = ts.cat_var\n",
    "cat_var_fut = ts.cat_fut\n",
    "num_var_past = ts.num_var+['x_num']\n",
    "import pickle \n",
    "with open('example.pkl', 'wb') as f:\n",
    "    pickle.dump([dataset,cat_var_past,cat_var_fut,num_var_past],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "459c491c-98ab-4513-b5eb-9f5c604b9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.to_csv('dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b53411e-8c81-4dc9-8e2e-c83c3a568be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning\n",
      "  Downloading lightning-2.3.3-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.1)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from lightning) (1.24.4)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from lightning) (23.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.0.0 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from lightning) (2.0.1)\n",
      "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
      "  Downloading torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from lightning) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from lightning) (4.12.2)\n",
      "Collecting pytorch-lightning (from lightning)\n",
      "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.1)\n",
      "Requirement already satisfied: setuptools in /Users/wenda/anaconda3/lib/python3.8/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (69.5.1)\n",
      "Requirement already satisfied: filelock in /Users/wenda/anaconda3/lib/python3.8/site-packages (from torch<4.0,>=2.0.0->lightning) (3.15.4)\n",
      "Requirement already satisfied: sympy in /Users/wenda/anaconda3/lib/python3.8/site-packages (from torch<4.0,>=2.0.0->lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/wenda/anaconda3/lib/python3.8/site-packages (from torch<4.0,>=2.0.0->lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from torch<4.0,>=2.0.0->lightning) (3.1.4)\n",
      "Collecting torch<4.0,>=2.0.0 (from lightning)\n",
      "  Downloading torch-2.4.1-cp38-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from jinja2->torch<4.0,>=2.0.0->lightning) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from sympy->torch<4.0,>=2.0.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/wenda/anaconda3/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\n",
      "Downloading lightning-2.3.3-py3-none-any.whl (808 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.5/808.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Downloading torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading torch-2.4.1-cp38-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightning-utilities, torch, torchmetrics, pytorch-lightning, lightning\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ragatouille 0.0.7.post11 requires langchain<0.2.0,>=0.1.0, but you have langchain 0.2.8 which is incompatible.\n",
      "ragatouille 0.0.7.post11 requires langchain_core<0.2.0,>=0.1.4, but you have langchain-core 0.2.19 which is incompatible.\n",
      "torchaudio 2.3.1 requires torch==2.3.1, but you have torch 2.4.1 which is incompatible.\n",
      "torchvision 0.15.2 requires torch==2.0.1, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed lightning-2.3.3 lightning-utilities-0.11.9 pytorch-lightning-2.4.0 torch-2.4.1 torchmetrics-1.5.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92518d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from typing import Union, List, Optional, Dict\n",
    "from sklearn.preprocessing import LabelEncoder  # Used for categorical encoding\n",
    "from datetime import datetime, timedelta  # For time-related operations\n",
    "import lightning as L  # PyTorch Lightning\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder  # Used for categorical encoding\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09307105",
   "metadata": {},
   "source": [
    "**Data Loading and Preparation (Example Data)**\n",
    "\n",
    "This section loads example time series data and prepares it for use in the `DataSet` classes. It demonstrates the structure of the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beea9295-9164-4bae-b2e1-8c5cc6c585c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.pkl', 'rb') as f:\n",
    "    dataset, cat_var_past, cat_var_fut, num_var_past = pickle.load(f)\n",
    "num_var_fut = []  # No numerical future variables in this example\n",
    "group = None       # No grouping in this first example.  Later examples *do* use grouping.\n",
    "time_var = 'time'\n",
    "target = ['signal']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fb3c8",
   "metadata": {},
   "source": [
    "**Example with Grouping**\n",
    "\n",
    "The following cell creates a second copy of the dataset and assigns it a different group ID. This is then concatenated with the original `dataset` to showcase how grouping works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db453ed2-df16-44bf-bc86-184fd4010e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset.copy()\n",
    "dataset2['group'] = 2  # Assign a different group ID\n",
    "dataset['group'] = 1   # Assign a group ID to the original\n",
    "dataset = pd.concat([dataset, dataset2])  # Concatenate to have multiple groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efaed82",
   "metadata": {},
   "source": [
    "**`extend_time_df` Function**\n",
    "This utility function extends a DataFrame to include all time steps within a specified range and frequency.  This is essential for ensuring that all time series have the same length and that there are no missing time steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3810de27-fc3b-44a3-b802-f88b9aa32c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_time_df(x:pd.DataFrame,time:str,freq:Union[str,int],group:Union[List[str],None]=None,global_minmax:bool=False)-> pd.DataFrame:\n",
    "    \"\"\"Extends a DataFrame to include all time steps within a range.\n",
    "\n",
    "    Ensures that the DataFrame `x` has rows for all time steps within the\n",
    "    range defined by the minimum and maximum values of the `time` column,\n",
    "    with a specified frequency `freq`.  Handles grouping if `group` is provided.\n",
    "\n",
    "    Args:\n",
    "        x: The input DataFrame. Must contain a 'time' column and any\n",
    "           columns specified in 'group'.\n",
    "        time: The name of the column containing the time index (e.g., 'time').\n",
    "        freq: The frequency to use for extending the DataFrame.  This can be\n",
    "            an integer (for integer time steps) or a string representing a\n",
    "            pandas frequency string (e.g., 'D' for daily, 'H' for hourly).\n",
    "        group:  Optional list of column names to group by. If provided, the\n",
    "            DataFrame will be extended separately for each group.\n",
    "        global_minmax: If True, use the global min/max of the 'time' column\n",
    "            across all groups. If False, use the min/max within each group.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: An extended DataFrame with all time steps within the\n",
    "            specified range and frequency.  Missing values introduced by the\n",
    "            extension are filled with NaNs.\n",
    "    \"\"\"\n",
    "    if group is None:\n",
    "        # No grouping: compute min and max time directly\n",
    "        if isinstance(freq, int):\n",
    "            empty = pd.DataFrame({'time': list(range(x[time].min(), x[time].max(), freq))})\n",
    "        else:\n",
    "            empty = pd.DataFrame({'time': pd.date_range(x[time].min(), x[time].max(), freq=freq)})\n",
    "\n",
    "    else:\n",
    "        # Grouping: compute min and max time for each group\n",
    "        if global_minmax:\n",
    "            # Use global min/max (across all groups)\n",
    "            _min = x[time].min()\n",
    "            _max = x[time].max()\n",
    "            _min_max = pd.DataFrame({'min': [_min], 'max': [_max]}) # Use a list for correct DataFrame creation\n",
    "            for c in group:\n",
    "                #  Create dummy columns, will be overwritten\n",
    "                _min_max[c] = _min_max['min']\n",
    "        else:\n",
    "            # Use per-group min/max\n",
    "            _min = x.groupby(group)[time].min().reset_index().rename(columns={time: 'min'})\n",
    "            _max = x.groupby(group)[time].max().reset_index().rename(columns={time: 'max'})\n",
    "            _min_max = pd.merge(_min, _max)\n",
    "\n",
    "        empty = []\n",
    "        for _, row in _min_max.iterrows():\n",
    "            if isinstance(freq, int):\n",
    "                tmp = pd.DataFrame({time: np.arange(row['min'], row['max'], freq)})\n",
    "            else:\n",
    "                tmp = pd.DataFrame({time: pd.date_range(row['min'], row['max'], freq=freq)})\n",
    "            for c in group:\n",
    "                tmp[c] = row[c]  # Assign the group values\n",
    "            empty.append(tmp)\n",
    "\n",
    "        empty = pd.concat(empty, ignore_index=True)  # Concatenate all group DataFrames\n",
    "    return empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc922a",
   "metadata": {},
   "source": [
    "**`_coerce_to_list` Function (Utility)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "217f2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coerce_to_list(obj):\n",
    "    \"\"\"Coerces input to a list.\n",
    "\n",
    "    Ensures that the input is always a list.  If the input is `None`,\n",
    "    it returns an empty list.  If the input is a single value (not already\n",
    "    a list), it returns a list containing that single value.\n",
    "\n",
    "    Args:\n",
    "        obj: The input object.\n",
    "\n",
    "    Returns:\n",
    "        list: A list representing the input object.\n",
    "    \"\"\"\n",
    "    if obj is None:\n",
    "        return []\n",
    "    if isinstance(obj, list):\n",
    "        return obj\n",
    "    else:\n",
    "        return [obj]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c65daa",
   "metadata": {},
   "source": [
    "**`PandasTSDataSet_MINIMAL` Class (Simplified Example)**\n",
    "This class is a simplified version of the `PandasTSDataSet` to illustrate the basic principles without the complexity of handling all the different data types and metadata.  It serves as a stepping stone to the full implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd01d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##TODO @Sandeep please try to replicate what is in PandasTSDataSet but for a smore compact example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96d5d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasTSDataSet_MINIMAL:\n",
    "    \"\"\"A simplified version of PandasTSDataSet for demonstration.\n",
    "\n",
    "    This class demonstrates the core functionality of creating a PyTorch\n",
    "    Dataset from a pandas DataFrame.  It handles:\n",
    "\n",
    "    - Extending the DataFrame to ensure regular time intervals.\n",
    "    - Returning tensors for time, target, and numerical features.\n",
    "    - Identifying valid data points (non-NaN).\n",
    "\n",
    "    It does *not* handle:\n",
    "    - Categorical features.\n",
    "    - Static features.\n",
    "    - Future known covariates.\n",
    "    - Grouping.\n",
    "    - Metadata handling (beyond a very basic level).\n",
    "\n",
    "    This simplified version is intended for educational purposes, to\n",
    "    illustrate the key concepts before diving into the full complexity\n",
    "    of the `PandasTSDataSet` class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        time: Optional[str] = None,\n",
    "        target: Optional[Union[str, List[str]]] = None,\n",
    "        num: Optional[List[Union[str, List[str]]]] = None,\n",
    "\n",
    "    ):\n",
    "        \"\"\"Initializes the PandasTSDataSet_MINIMAL.\n",
    "\n",
    "        Args:\n",
    "            data: The input pandas DataFrame.\n",
    "            time: The name of the time index column.\n",
    "            target: The name(s) of the target variable column(s).\n",
    "            num: The name(s) of numerical feature columns.\n",
    "        \"\"\"\n",
    "\n",
    "        self.time = time\n",
    "        self.target = _coerce_to_list(target)\n",
    "        self.num = _coerce_to_list(num)\n",
    "        self.data = data.copy()\n",
    "\n",
    "\n",
    "        self.feature_cols = self.num\n",
    "        #use set ensuring unique columns\n",
    "        self.data = data[list(set(self.feature_cols + self.target + [self.time]))].copy()\n",
    "\n",
    "\n",
    "        self.label_encoders = {}\n",
    "\n",
    "        ##ensure to have a coherent dataset\n",
    "        self.data.drop_duplicates(subset= [time],keep='first',inplace=True,ignore_index=True)\n",
    "\n",
    "        ##compute minumum frequency\n",
    "        freq = self.data['time'].diff.min()\n",
    "\n",
    "\n",
    "        if isinstance(freq, timedelta):\n",
    "            freq = pd.to_timedelta(freq)\n",
    "        elif isinstance(freq,  (int, float)):\n",
    "            freq = int(freq)\n",
    "        else:\n",
    "            raise TypeError(\"time must be integer or datetime\")\n",
    "\n",
    "\n",
    "        ##extend dataset\n",
    "        self.data = extend_time_df(self.data,self.time,freq,None).merge(self.data,how='left').reset_index()\n",
    "\n",
    "\n",
    "        self._groups = {\"_single_group\": self.data.index}\n",
    "        self._group_ids = [\"_single_group\"]\n",
    "\n",
    "        ## we know on the fly which rows are valid and wich contains nans\n",
    "        self.data['valid'] = ~pd.isnull(self.data.max(axis=1))\n",
    "\n",
    "        self._prepare_metadata()\n",
    "\n",
    "    def _prepare_metadata(self):\n",
    "        \"\"\"Prepare metadata for the dataset.\"\"\"\n",
    "        self.metadata = {\n",
    "            \"cols\": {\n",
    "                \"y\": self.target,\n",
    "                \"x\": self.feature_cols,\n",
    "                \"st\": [],  # No static features in this minimal version\n",
    "            },\n",
    "            \"col_type\": {},\n",
    "            \"col_known\": {},\n",
    "            \"cat_index\":[] # No categorical features in this minimal version\n",
    "        }\n",
    "        # Removed the loop that checks for categorical features, as they are not handled here.\n",
    "\n",
    "        all_cols = self.target + self.feature_cols\n",
    "        for col in all_cols:\n",
    "            self.metadata[\"col_type\"][col] = \"F\"  # All features are numerical (\"F\")\n",
    "\n",
    "            self.metadata[\"col_known\"][col] = \"K\" if col in self.feature_cols else \"U\"\n",
    "        self.metadata['encoders'] = self.label_encoders # No encoders, as they're used for categoricals\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return number of time series in the dataset.\"\"\"\n",
    "        return len(self._group_ids)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Get time series data for given index.\n",
    "\n",
    "        Args:\n",
    "            index: The index of the time series to retrieve (not used directly,\n",
    "                as we have a single time series in this simplified version).\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing tensors for 't', 'y', 'x', and 'is_valid'.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "        data = self.data.sort_values(by=self.time)\n",
    "\n",
    "        result = {\n",
    "            \"t\": data[self.time].values,  # Return as numpy array (no need for tensor here)\n",
    "            \"y\": torch.tensor(data[self.target].values,dtype=torch.float32,device=device),\n",
    "            \"x\": torch.tensor(data[self.feature_cols].values,dtype=torch.float32,device=device),\n",
    "            'is_valid': torch.tensor(data.valid.values,dtype=torch.int,device=device), #indicator of if data is valid\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    def get_metadata(self) -> Dict:\n",
    "        \"\"\"Return metadata about the dataset.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing metadata about the columns.\n",
    "        \"\"\"\n",
    "        return self.metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929cd13",
   "metadata": {},
   "source": [
    "**`PandasTSDataSet` Class (D1 - pandas Implementation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "761d3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasTSDataSet(Dataset):\n",
    "    \"\"\"PyTorch Dataset for loading time series data from a pandas DataFrame.\n",
    "\n",
    "    This class implements the D1 (raw data) layer of the `pytorch-forecasting`\n",
    "    v2 API.  It handles loading time series data from a pandas DataFrame,\n",
    "    performing basic preprocessing (like ensuring regular time intervals),\n",
    "    and providing access to the data via the `__getitem__` method.\n",
    "\n",
    "    Key Features:\n",
    "    - Handles multiple time series (identified by group IDs).\n",
    "    - Supports numerical and categorical features.\n",
    "    - Supports static features (features that don't change over time).\n",
    "    - Supports known and unknown covariates.\n",
    "    - Automatically extends the DataFrame to ensure regular time intervals.\n",
    "    - Returns data in a standardized dictionary format (tensors).\n",
    "    - Provides metadata about the columns (names, types, known/unknown status).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        time: Optional[str] = None,\n",
    "        target: Optional[Union[str, List[str]]] = None,\n",
    "        group: Optional[List[str]] = None,\n",
    "        num: Optional[List[Union[str, List[str]]]] = None,\n",
    "        cat: Optional[List[Union[str, List[str]]]] = None,\n",
    "        known: Optional[List[Union[str, List[str]]]] = None,\n",
    "        unknown: Optional[List[Union[str, List[str]]]] = None,\n",
    "        static: Optional[List[Union[str, List[str]]]] = None,\n",
    "        label_encoders: Optional[dict] = None\n",
    "    ):\n",
    "        \"\"\"Initializes the PandasTSDataSet.\n",
    "\n",
    "        Args:\n",
    "            data: The input pandas DataFrame.\n",
    "            time: The name of the time index column.\n",
    "            target: The name(s) of the target variable column(s).\n",
    "            group:  List of column names identifying a time series instance.\n",
    "            num: The name(s) of numerical feature columns.\n",
    "            cat: The name(s) of categorical feature columns.\n",
    "            known:  List of variables known in the future.\n",
    "            unknown: List of variables unknown in the future\n",
    "            static: The name(s) of static feature columns (features that don't\n",
    "                change over time).\n",
    "            label_encoders: Optional dictionary of pre-fitted label encoders\n",
    "                for categorical features. If not provided, new encoders will\n",
    "                be fitted.\n",
    "        \"\"\"\n",
    "\n",
    "        self.time = time\n",
    "        self.target = _coerce_to_list(target)\n",
    "        self.num = _coerce_to_list(num)\n",
    "        self.cat = _coerce_to_list(cat)\n",
    "        self.known = _coerce_to_list(known)\n",
    "        self.unknown = _coerce_to_list(unknown)\n",
    "        self.static = _coerce_to_list(static)\n",
    "        self.data = data.copy()\n",
    "\n",
    "        self.group = _coerce_to_list(group)\n",
    "\n",
    "\n",
    "        self.feature_cols = self.num + self.cat\n",
    "        #use set ensuring unique columns\n",
    "        self.data = data[list(set(self.static + self.feature_cols + self.target + [self.time] +self.group))].copy()\n",
    "\n",
    "        ##Encoders for categorical since we want to return tensors\n",
    "        if label_encoders is None:\n",
    "            label_encoders = {}\n",
    "            ##Encoders for categorical since we want to return tensors\n",
    "            for c in self.cat+self.group:\n",
    "                label_encoders[c] = OrdinalEncoder()\n",
    "                self.data[c] = label_encoders[c].fit_transform(self.data[c].values.reshape(-1,1)).flatten()\n",
    "            self.label_encoders = label_encoders\n",
    "        else:\n",
    "            for c in self.cat+self.group:\n",
    "                self.data[c] = label_encoders[c].transform(self.data[c].values.reshape(-1,1)).flatten()\n",
    "            self.label_encoders = label_encoders\n",
    "\n",
    "\n",
    "        ##ensure to have a coherent dataset\n",
    "        self.data.drop_duplicates(subset= self.group+[self.time],keep='first',inplace=True,ignore_index=True)\n",
    "\n",
    "        ##compute minumum frequency\n",
    "        if self.group is None:\n",
    "            freq = self.data[self.time].diff.min()\n",
    "        else:\n",
    "            freq = self.data.groupby(self.group).time.diff().min()\n",
    "\n",
    "        if isinstance(freq, timedelta):\n",
    "            freq = pd.to_timedelta(freq)\n",
    "        elif isinstance(freq,  (int, float)):\n",
    "            freq = int(freq)\n",
    "        else:\n",
    "            raise TypeError(\"time must be integer or datetime\")\n",
    "\n",
    "\n",
    "        ##extend dataset\n",
    "        self.data = extend_time_df(self.data,self.time,freq,self.group).merge(self.data,how='left').reset_index()\n",
    "\n",
    "\n",
    "        ##now we are sure that data is in a coherent form!\n",
    "        self.lengths  = {}\n",
    "        if self.group:\n",
    "            self._groups = self.data.groupby(self.group).groups\n",
    "            self._group_ids =  list(self._groups.keys())\n",
    "            for k in self._groups:\n",
    "                self.lengths[k] = len(self._groups[k])\n",
    "        else:\n",
    "            self._groups = {0: self.data.index}\n",
    "            self._group_ids = [0]\n",
    "            self.lengths[0] = len( self.data.index)\n",
    "\n",
    "\n",
    "\n",
    "        ## we know on the fly which rows are valid and wich contains nans\n",
    "        self.data['valid'] = ~pd.isnull(self.data.max(axis=1))\n",
    "\n",
    "        self._prepare_metadata()\n",
    "\n",
    "    def _prepare_metadata(self):\n",
    "        \"\"\"Prepare metadata for the dataset.\"\"\"\n",
    "        self.metadata = {\n",
    "            \"cols\": {\n",
    "                \"y\": self.target,\n",
    "                \"x\": self.feature_cols,\n",
    "                \"st\": self.static,\n",
    "            },\n",
    "            \"col_type\": {},\n",
    "            \"col_known\": {},\n",
    "            \"cat_index\":[]\n",
    "        }\n",
    "        for i, c in enumerate(self.feature_cols):\n",
    "            if c in self.cat:\n",
    "                self.metadata['cat_index'].append(i) # EDIT: was getting error with i, so updated enumerate in for loop\n",
    "                \n",
    "\n",
    "        all_cols = self.target + self.feature_cols + self.static\n",
    "        for col in all_cols:\n",
    "            self.metadata[\"col_type\"][col] = \"C\" if col in self.cat else \"F\"\n",
    "            self.metadata[\"col_known\"][col] = \"K\" if col in self.known else \"U\"\n",
    "        self.metadata['encoders'] = self.label_encoders\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return number of time series in the dataset.\"\"\"\n",
    "        return len(self._group_ids)\n",
    "\n",
    "    def get_id_ts_by_idx(self,idx):\n",
    "        \"\"\"Gets the time series ID and starting index for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx: The overall index.  This is *not* the group index, but\n",
    "                 the index considering all time series concatenated together.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the group ID and the starting index\n",
    "                of that group in the concatenated data.\n",
    "        \"\"\"\n",
    "        tmp = np.cumsum(list(self.lengths.values()))\n",
    "        idx_group =  min(np.where(tmp>idx)[0])  # EDIT: Find the group index, as we are getting group\n",
    "        return idx_group, 0 if idx_group==0 else tmp[idx_group-1] #EDIT: group id, index of the start of the series\n",
    "\n",
    "    def get_total_len(self):\n",
    "        \"\"\"Returns total length of the dataset\"\"\"\n",
    "        l = 0\n",
    "        for k in  self._groups:\n",
    "            l+= self.lengths[k]\n",
    "        return l\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Get time series data for given index.\n",
    "\n",
    "        Args:\n",
    "            index: The index of the time series to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing tensors for 't', 'y', 'x', 'group', 'st',\n",
    "            and 'is_valid'.\n",
    "        \"\"\"\n",
    "        group_id = self._group_ids[index]\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if self.group:\n",
    "            mask = self._groups[group_id]\n",
    "            data = self.data.loc[mask].sort_values(by=self.time)\n",
    "        else:\n",
    "            data = self.data.sort_values(by=self.time)\n",
    "\n",
    "        result = {\n",
    "            \"t\": data[self.time].values,  # Return as numpy array\n",
    "            \"y\": torch.tensor(data[self.target].values, dtype=torch.float32, device=device),\n",
    "            \"x\": torch.tensor(data[self.feature_cols].values, dtype=torch.float32, device=device),\n",
    "            \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
    "            \"st\": torch.tensor(data[self.static].iloc[0].values, dtype=torch.float32, device=device) if self.static else torch.tensor([], dtype=torch.float32, device=device), # return empty if no statics\n",
    "            'is_valid': torch.tensor(data.valid.values,dtype=torch.int,device=device), #indicator of if data is valid\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_metadata(self) -> Dict:\n",
    "        \"\"\"Return metadata about the dataset.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        Dict\n",
    "            Dictionary containing:\n",
    "            - cols: column names for y, x, and static features\n",
    "            - col_type: mapping of columns to their types (F/C)\n",
    "            - col_known: mapping of columns to their future known status (K/U)\n",
    "            - encoders: Dict of label encoders used\n",
    "        \"\"\"\n",
    "        return self.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030144e",
   "metadata": {},
   "source": [
    "**Configuration dictionary for the data module.  This holds hyperparameters related to data splitting, preprocessing, and windowing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "290ab40d-f440-4f7f-9ae9-8855a04f1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module_metadata = dict(\n",
    "  perc_train= 0.7,       # Percentage of data for training\n",
    "  perc_valid= 0.1,       # Percentage of data for validation\n",
    "  range_train= None,     # Optional: Specific time range for training (not used here)\n",
    "  range_validation= None,  # Optional: Specific time range for validation (not used here)\n",
    "  range_test= None,      # Optional: Specific time range for testing (not used here)\n",
    "  shift= 0,              # Optional: Shift the time series (not used here)\n",
    "  starting_point= None,  # Optional: Starting point for sampling (not used here)\n",
    "  skip_step= 1,          # Optional: Step size for skipping data points (not used here)\n",
    "  past_steps=16,         # Number of past time steps to use as input (encoder length)\n",
    "  future_steps= 16,      # Number of future time steps to predict (decoder length)\n",
    "  precompute=True,      # Whether to precompute the train/val/test splits.\n",
    "  scaler= 'sklearn.preprocessing.StandardScaler()'  # String representation of the scaler (not used directly here)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611fbc0",
   "metadata": {},
   "source": [
    "**Instantiate the PandasTSDataSet (D1 layer).**\n",
    "This creates the dataset object that will provide the raw time series data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4ca4055-7f36-446c-9b21-6c0456b152cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PandasTSDataSet(dataset,\n",
    "                     'time',  # Name of the time index column\n",
    "                     target,  # List of target variable names\n",
    "                     'group', # Name of the column that identifies groups/time series\n",
    "                     list(set(num_var_past+num_var_fut)),  # List of numerical feature columns\n",
    "                     list(set(cat_var_fut+cat_var_past)),  # List of categorical feature columns\n",
    "                     list(set(cat_var_fut+num_var_fut)),  # List of future known covariates\n",
    "                     list(set(cat_var_past+num_var_past)), # List of past known covariates\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9edfd",
   "metadata": {},
   "source": [
    "Import necessary PyTorch modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "519ebe66-3d64-41df-a191-81c0ba518e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0375ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    \"\"\"Custom collate function to handle potential None values in batches.\n",
    "\n",
    "    Filters out `None` values from the batch before collating.  This is\n",
    "    necessary because the `MyDataset.__getitem__` method can return `None`\n",
    "    if a valid window cannot be created for a given index.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of samples (dictionaries or tensors) from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        The collated batch, with `None` values removed.\n",
    "    \"\"\"\n",
    "    batch = list(filter(lambda x : x is not None, batch))\n",
    "    #print(len(batch))  # Debugging print statement (removed in final version)\n",
    "    return default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e31a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: @SANDEEP rework this using some cleaner code you can find here\n",
    "#https://colab.research.google.com/drive/1FvLlmEOgm3D3JgNFVeAtwPk4cXagJ0CY?usp=sharing#scrollTo=jFuSwWrhTg6y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12bc5f1c-2de1-4837-9d2a-e764fa395bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for time series data.\n",
    "\n",
    "    This class adapts the D1 dataset (`PandasTSDataSet`) for use with a\n",
    "    PyTorch `DataLoader`.  It handles the creation of input/output windows\n",
    "    (sequences) for training and validation.  It also handles the case where\n",
    "    a valid window cannot be created (e.g., due to insufficient past data)\n",
    "    by returning `None`.\n",
    "\n",
    "    Args:\n",
    "        data: The D1 dataset instance (e.g., `PandasTSDataSet`).\n",
    "        metadata: A dictionary containing metadata about the dataset and\n",
    "            data processing parameters (e.g., `past_steps`, `future_steps`).\n",
    "        valid_index:  A dictionary where keys are group IDs and values are\n",
    "            lists of valid indices for that group.  This defines the\n",
    "            valid ranges within each time series for creating windows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, metadata, valid_index) -> torch.utils.data.Dataset:\n",
    "        \"\"\"Initializes the MyDataset instance.\"\"\"\n",
    "\n",
    "        self.metadata = metadata\n",
    "        self.metadata_dataset = data.metadata  # Metadata from the D1 dataset\n",
    "        self.data = data  # The D1 dataset instance\n",
    "        self.valid_index = valid_index  # Dictionary of valid indices for each group\n",
    "        sum = 0\n",
    "        self.lengths = {}\n",
    "        #fix this, probably we need to add something (check validity, check past and future)\n",
    "        for k in self.valid_index:\n",
    "            # Calculate the total number of valid windows that can be created\n",
    "            # from each group.  Subtract `future_steps` because we need enough\n",
    "            # data points for both the input window and the target window.\n",
    "            sum+=(len(self.valid_index[k])-self.metadata['future_steps'])\n",
    "            self.lengths[k] = len(self.valid_index[k])\n",
    "        self.length = sum # Sum of lengths accross differnet time series.\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of valid windows in the dataset.\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def get_id_ts_by_idx(self,idx):\n",
    "        \"\"\"Gets time series id given global index\"\"\"\n",
    "        tmp = np.cumsum(list(self.lengths.values()))\n",
    "        idx =  min(np.where(tmp>idx)[0])\n",
    "        return idx, 0 if idx==0 else tmp[idx-1]\n",
    "\n",
    "    def __getitem__(self, idxs):\n",
    "        \"\"\"Retrieves a sample (input/output window) from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idxs: The *global* index of the window to retrieve.  This index\n",
    "                is across all time series in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the input and output tensors for the window,\n",
    "            or `None` if a valid window cannot be created at the given index.\n",
    "        \"\"\"\n",
    "\n",
    "        ##crucial point there: correctly identifying which are the indexes that we need!\n",
    "        sample = {}\n",
    "        IDX,difference = self.get_id_ts_by_idx(idxs) #find group and local index\n",
    "\n",
    "        idxs -= difference  # Adjust index to be relative to the start of the time series\n",
    "        idxs = self.valid_index[IDX][idxs]  # Get the actual index in the data\n",
    "        tmp = self.data.__getitem__(IDX) ##use the getitem of the data!\n",
    "\n",
    "        # Check if enough data is available for both input and output windows\n",
    "        if idxs+self.metadata['future_steps']>self.data.lengths[IDX]:\n",
    "            return None  # Not enough data, return None\n",
    "\n",
    "        #Check for the validity of the time series, by checking nans.\n",
    "        if tmp['is_valid'][idxs-self.metadata['past_steps']:idxs+self.metadata['future_steps']].sum() != self.metadata['future_steps']+self.metadata['past_steps']:\n",
    "          return None\n",
    "      \n",
    "        # Construct sample\n",
    "        for k in tmp.keys():\n",
    "            # Separate based on past/future\n",
    "            # We create past and future samples only for the ones we sliced using idxs\n",
    "            if '_past' in k:\n",
    "                sample[k] = tmp[k][idxs-self.metadata['past_steps']:idxs] #past steps based on the past_step provided in metadata.\n",
    "            elif '_fut' in k or k=='y':\n",
    "                sample[k] = tmp[k][idxs:idxs+self.metadata['future_steps']]# future steps based on the future_step\n",
    "            else:\n",
    "                pass\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac4a66d5-753b-4030-a88d-e5b8efd73d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranges(d1_dataset, metadata):\n",
    "    \"\"\"Computes valid index ranges for training, validation, and testing.\n",
    "\n",
    "    This function calculates the start and end indices for the training,\n",
    "    validation, and testing sets within each time series group, based on the\n",
    "    provided percentages in `metadata`.\n",
    "\n",
    "    Args:\n",
    "        d1_dataset: The D1 dataset instance (e.g., `PandasTSDataSet`).\n",
    "        metadata: A dictionary containing metadata, including `perc_train`\n",
    "            and `perc_valid` (percentages for training and validation splits).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three dictionaries: `train_ranges`,\n",
    "            `valid_ranges`, and `test_ranges`.  Each dictionary has group IDs\n",
    "            as keys and lists of valid indices as values.\n",
    "    \"\"\"\n",
    "    ##suppose for now we use only percentage!\n",
    "    train_ranges = {}\n",
    "    valid_ranges = {}\n",
    "    test_ranges = {}\n",
    "    for k in d1_dataset.lengths:  # Iterate over each time series group\n",
    "        ls = d1_dataset.lengths[k]  # Get the length of the current time series\n",
    "        train_ranges[k] = list(range(0,int(metadata['perc_train']*ls)))  # Training range\n",
    "        valid_ranges[k] = list(range(int(metadata['perc_train']*ls), int((metadata['perc_valid'] + metadata['perc_train'])*ls)))  # Validation range\n",
    "        test_ranges[k] = list(range(int((metadata['perc_valid'] + metadata['perc_train'])*ls),ls))  # Testing range\n",
    "\n",
    "    ## in case of datetime ranges we need to use np.where and create a mask\n",
    "    return train_ranges,valid_ranges,test_ranges\n",
    "\n",
    "# Calculate the training, validation, and testing ranges\n",
    "train_ranges, valid_ranges, test_ranges = compute_ranges(ds, data_module_metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185143c",
   "metadata": {},
   "source": [
    "**Create a DataLoader for the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e43bfd7-12b3-48c2-8b33-d5ca6681959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl =  DataLoader(\n",
    "            MyDataset(ds, data_module_metadata, train_ranges),  # Create a MyDataset instance\n",
    "            batch_size=32,  # Set the batch size\n",
    "            collate_fn=my_collate  # Use the custom collate function\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dde2ba1f-f6f8-483c-a404-5917b84ce37a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n",
      "/var/folders/9z/1zsbrjl17xj_76sy78736hmr0000gn/T/ipykernel_39262/1986820013.py:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"group\": torch.tensor([group_id], dtype=torch.int64, device=device),  # group ID, cast to int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example of iterating through the DataLoader (for debugging/demonstration).\n",
    "for x in dl:\n",
    "    print(x['y'].shape)  # Print the shape of the target variable tensor.\n",
    "    break #only check first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddeb11",
   "metadata": {},
   "source": [
    "**The following function and class show how one might create a `LightningDataModule` to manage the data loading process, including train/validation/test splits. This builds on top of previous components.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86c9c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderEncoderDataModule(L.LightningDataModule):\n",
    "    \"\"\"LightningDataModule for encoder-decoder models.\n",
    "\n",
    "    This class manages the data loading and preprocessing for training,\n",
    "    validation, testing, and prediction.  It handles splitting the data,\n",
    "    creating `Dataset` instances, and creating `DataLoader` instances.  It\n",
    "    supports both precomputed splits and on-the-fly splitting.\n",
    "\n",
    "    Args:\n",
    "        d1_dataset: The D1 dataset instance (e.g., `PandasTSDataSet`).\n",
    "        batch_size: The batch size.\n",
    "        num_workers: The number of worker processes for data loading.\n",
    "        metadata: A dictionary containing metadata and hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, d1_dataset, batch_size=32, num_workers=4, metadata=None):\n",
    "        super().__init__()\n",
    "        # initialize other  params\n",
    "        self.d1_dataset = d1_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.metadata = metadata\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepares the data for training/validation/testing.\n",
    "\n",
    "        This method handles splitting the data into training, validation, and\n",
    "        testing sets. It supports two modes:\n",
    "\n",
    "        - `precompute=True`:  Uses a `split_data` function (not defined here,\n",
    "          but assumed to exist) to precompute the splits. This is useful for\n",
    "          deterministic splits or when the splitting process is expensive.\n",
    "        - `precompute=False`:  Calculates the splits on-the-fly using\n",
    "          `compute_ranges`. This is more flexible and allows for different\n",
    "          splitting strategies.\n",
    "        \"\"\"\n",
    "        if self.metadata['precompute']:\n",
    "            self.precompute = True\n",
    "            ## @SANDEEP this is what is returned by DSIPTS split_for_train!!\n",
    "            ## the only difference is that we need to use the __getitem__ of the d1 layer\n",
    "\n",
    "            # The split_data function is assumed to be defined elsewhere and\n",
    "            # would handle the splitting logic according to however the\n",
    "            # project defines \"splitting\" (could be by time, by groups, etc.)\n",
    "            self.train_dataset, self.validation_dataset, self.test_dataset = split_data(self.d1_dataset,self.metadata) # Placeholder\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            self.precompute = False\n",
    "            ##in this case we need to pass only the data that are referring to the trainin period\n",
    "            ## but data can be chunked, we need to create a function that transform a D1 object into a D1 object or, even better\n",
    "            ## something that can be used by the dataset for filtering only valid samples!\n",
    "            ##for example\n",
    "            self.train_ranges, self.validation_ranges, self.test_ranges = compute_ranges(self.d1_dataset,self.metadata)\n",
    "            self.predict_data = None ##??? need to study this\n",
    "            ##TODO normalization???? maybe we can precompute here some statistics and pass them to the dataset!\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Sets up the datasets for each stage (fit, test, predict).\n",
    "\n",
    "        This method creates the appropriate `Dataset` instances for each stage,\n",
    "        using either the precomputed splits or the on-the-fly calculated ranges.\n",
    "\n",
    "        Args:\n",
    "            stage: The stage ('fit', 'test', or 'predict').\n",
    "        \"\"\"\n",
    "        # Get metadata from D1 layer during setup\n",
    "        #self.metadata = self.d1_dataset.get_metadata() ##NO SEE COMMENT BEFORE!  We get metadata in __init__\n",
    "        ##create dataset here\n",
    "        if stage == 'fit':\n",
    "            if self.precompute:\n",
    "                # Use precomputed datasets (implementation of split_data needed)\n",
    "                pass # Use self.train_dataset and self.validation_dataset as defined in prepare_data\n",
    "            else:\n",
    "                # Create datasets using computed ranges\n",
    "                self.train_dataset = MyDataset(self.d1_dataset, self.metadata, self.train_ranges) # Pass train_ranges\n",
    "                self.validation_dataset = MyDataset(self.d1_dataset, self.metadata, self.validation_ranges) # Pass valid_ranges\n",
    "\n",
    "        if stage == 'test':\n",
    "            if self.precompute:\n",
    "              pass\n",
    "                # Use precomputed test dataset\n",
    "            else:\n",
    "                self.test_dataset = MyDataset(self.d1_dataset, self.metadata, self.test_ranges) # Pass test_ranges\n",
    "\n",
    "        if stage == 'predict':\n",
    "            if self.precompute:\n",
    "                pass\n",
    "                # Use precomputed prediction dataset (if applicable)\n",
    "            else:\n",
    "                self.predict_datasset = MyDataset(self.d1_dataset, self.metadata, self.predict_data) # Pass predict_data.  Implementation details TBD.\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Returns the training DataLoader.\"\"\"\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=my_collate)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Returns the validation DataLoader.\"\"\"\n",
    "        return DataLoader(self.validation_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=my_collate)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"Returns the test DataLoader.\"\"\"\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=my_collate)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        \"\"\"Returns the prediction DataLoader.\"\"\"\n",
    "        return DataLoader(self.predict_datasset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=my_collate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83526e8",
   "metadata": {},
   "source": [
    "**`DecoderEncoderModel` Class (M Layer - Example)**\n",
    "This is a placeholder for the Model Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d9a07bd-35cd-4cbd-8843-081b7d3d8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Layer M - Example (Placeholder)\n",
    "class DecoderEncoderModel(L.LightningModule):\n",
    "    \"\"\"Placeholder for an encoder-decoder model (M layer).\n",
    "\n",
    "    This is a minimal example of how a model might be structured using\n",
    "    PyTorch Lightning.  It shows how to access the metadata from the\n",
    "    `LightningDataModule`.  A real implementation would include the actual\n",
    "    network architecture and training logic.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metadata = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Gets metadata from the datamodule during setup.\"\"\"\n",
    "        # Get metadata from datamodule during setup\n",
    "        self.metadata = self.trainer.datamodule.metadata\n",
    "\n",
    "        # Initialize layer T model using metadata (placeholder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass (placeholder).\"\"\"\n",
    "        # forward logic (implementation would go here)\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step (placeholder).\"\"\"\n",
    "        pass  # Replace with actual training logic\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validation step (placeholder).\"\"\"\n",
    "        pass # Replace with actual validation logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80542685",
   "metadata": {},
   "source": [
    "### Layer M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "694a020f-7f8d-4503-a023-bfdcda90667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DecoderEncoderModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metadata = None\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        # Get metadata from datamodule during setup\n",
    "        self.metadata = self.trainer.datamodule.metadata\n",
    "        \n",
    "        # Initialize layer T model using metadata\n",
    "    \n",
    "    def forward(self, x):\n",
    "     # forward logic\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43226c-f99f-4a77-8793-c3e9db5c4536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
